{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project: Automated Essay Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitted by - Gupta, Amit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Introduction__ Automated Essay Scoring (AES) systems are targeted at both alleviating the workload of teachers and improving the feedback cycle in educational systems. AES systems have also\n",
    "seen adoption for several high-stakes assessment, e.g., the e-rater system which has been used for TOEFL and GRE examinations. A successful AES system brings about widespread benefits to society and the education industry. Traditionally, the task of AES has been regarded as a machine learning problem which learns to approximate the marking process with supervised learning. Decades of AES research follow the same traditional supervised text regression methods in which handcrafted features are constructed and subsequently passed into a machine learning based classifier. A wide assortment of features are commonly extracted from essays. Simple and intuitive features may include essay length, sentence length. On the other hand, intricate and complex features may also be extracted, e.g.., features such as grammar correctness , readability  and textual\n",
    "coherence. However, these handcrafted features are often painstakingly designed, require a lot of human involvement and usually require laborious implementation for every new feature.\n",
    "\n",
    "In this project we will apply deep learning models to learn important features of the essay automatically, and compare the results with the state of art deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dataset Description__: There are eight essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.\n",
    "\n",
    "The training data is provided in three formats: a tab-separated value (TSV) file, a Microsoft Excel 2010 spreadsheet, and a Microsoft Excel 2003 spreadsheet.  The current release of the training data contains essay sets 1-6.  Sets 7-8 will be released on February 10, 2012.  Each of these files contains 28 columns:\n",
    "\n",
    "essay_id: A unique identifier for each individual student essay\n",
    "essay_set: 1-8, an id for each set of essays\n",
    "essay: The ascii text of a student's response\n",
    "rater1_domain1: Rater 1's domain 1 score; all essays have this\n",
    "rater2_domain1: Rater 2's domain 1 score; all essays have this\n",
    "rater3_domain1: Rater 3's domain 1 score; only some essays in set 8 have this.\n",
    "domain1_score: Resolved score between the raters; all essays have this\n",
    "rater1_domain2: Rater 1's domain 2 score; only essays in set 2 have this\n",
    "rater2_domain2: Rater 2's domain 2 score; only essays in set 2 have this\n",
    "domain2_score: Resolved score between the raters; only essays in set 2 have this\n",
    "rater1_trait1 score - rater3_trait6 score: trait scores for sets 7-8\n",
    "\n",
    "__We are using tsv training data file, and this project is focussed on one essay prompt namely \"essay set 3\". This is one of the harder essays to train judging by various papers. Please see the following 2 papers for all the previous work, and the state of art results.__\n",
    "* Incorporating Neural Coherence Features: https://arxiv.org/abs/1711.04981\n",
    "* A Siamese Bidirectional LSTM Architecture: https://www.mdpi.com/2073-8994/10/12/682\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Deep Learning Model__: Following are the list of techniques applied in this project\n",
    "* Vectorization (tf-idf, and jointly learned embeddings) \n",
    "* Networks: Dense, CNN+Dense, LSTM+Dense, BiLSTM+Dense, CNN+LSTM+Dense, CNN+BiLSTM+Dense\n",
    "* Novel NLP data augmentation techniques (not covered in class)\n",
    "* * __Synonym Replacement__ (SR): Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\n",
    "* * __Random Insertion__ (RI): Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.\n",
    "* * __Random Swap__ (RS): Randomly choose two words in the sentence and swap their positions. Do this n times.\n",
    "* * __Random Deletion__ (RD): For each word in the sentence, randomly remove it with probability p.\n",
    "\n",
    "To keep this notebook concise, I have shown only the results of CNN+Dense network as this \n",
    "network showed the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Overall Results for CNN+Dense DL model__ \n",
    "* Without data augmentation: Kappa is 0.48 (for tf-idf) - 0.51 (with embeddings)\n",
    "* With data augmentation: Kappa improved to 0.76 for both tf-idf and embeddings\n",
    "* State of art results reported in the papers (without data augmentation) is 0.78 with Siamese BiLSTM model.\n",
    "\n",
    "__Results for LSTM based DL models__\n",
    "* Plain vanilla LSTM and BiLSTM models performed worse than CNN based models. I think the reason is essays are long (100 words), and these networks appear to not do a good job on remembering the long history.\n",
    "* Tweaks to LSTM and BiLSTM models are needed as shown in the promising work in the listed papers on neural coherence, and siamese bi-lstm approaches to get better results\n",
    "\n",
    "__SUMMARY__\n",
    "* Overall the results with data augmentation with simple model architecture comes close to state of art results.\n",
    "* Simple models can perform as well as advanced models (attention, neural coherence) given a large sample size to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bGlLjLj8ytZ8"
   },
   "source": [
    "### Set up directory constants\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKA9h1r-ytZ9"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = './data'\n",
    "NLTK_DIR = './'\n",
    "PUNKT_DIR = './punkt'\n",
    "STOPWORDS_DIR = './stopwords'\n",
    "WORDNET_DIR = './wordnet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 959,
     "status": "ok",
     "timestamp": 1564374529125,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "y7WaRkC59KRh",
    "outputId": "00a9fe24-d1af-4477-9c70-3c5a8b7150c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to ./punkt...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to ./stopwords...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to ./wordnet...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Add all the imports here\n",
    "\n",
    "#keras\n",
    "import keras\n",
    "from keras.layers import Embedding, Dense, Dropout\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "\n",
    "#numpy, pandas\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#nltk\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('punkt', download_dir=PUNKT_DIR)\n",
    "nltk.download('stopwords', download_dir=STOPWORDS_DIR)\n",
    "nltk.download('wordnet', download_dir=WORDNET_DIR)\n",
    "\n",
    "nltk.data.path.append(PUNKT_DIR)\n",
    "nltk.data.path.append(STOPWORDS_DIR)\n",
    "nltk.data.path.append(WORDNET_DIR)\n",
    "\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "\n",
    "#random\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RwDkhFCW-KEc"
   },
   "source": [
    "### Helper functions for data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWL_Kcub-Tfy"
   },
   "outputs": [],
   "source": [
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    pickle_path = NLTK_DIR+ '/punkt'+ '/tokenizers/punkt/english.pickle'\n",
    "    tokenizer = nltk.data.load(pickle_path)\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fCXtq9_vuRR7"
   },
   "source": [
    "### Data Description\n",
    "\n",
    "There are eight essay sets. Each of the sets of essays was generated from a single prompt. Selected essays range from an average length of 150 to 550 words per response. Some of the essays are dependent upon source information and others are not. All responses were written by students ranging in grade levels from Grade 7 to Grade 10. All essays were hand graded and were double-scored. Each of the eight data sets has its own unique characteristics. The variability is intended to test the limits of your scoring engine's capabilities.\n",
    "\n",
    "The training data is provided in three formats: a tab-separated value (TSV) file, a Microsoft Excel 2010 spreadsheet, and a Microsoft Excel 2003 spreadsheet.  The current release of the training data contains essay sets 1-6.  Sets 7-8 will be released on February 10, 2012.  Each of these files contains 28 columns:\n",
    "\n",
    "- essay_id: A unique identifier for each individual student essay\n",
    "- essay_set: 1-8, an id for each set of essays\n",
    "- essay: The ascii text of a student's response\n",
    "- rater1_domain1: Rater 1's domain 1 score; all essays have this\n",
    "- rater2_domain1: Rater 2's domain 1 score; all essays have this\n",
    "- rater3_domain1: Rater 3's domain 1 score; only some essays in set 8 have this.\n",
    "- domain1_score: Resolved score between the raters; all essays have this\n",
    "- rater1_domain2: Rater 1's domain 2 score; only essays in set 2 have this\n",
    "- rater2_domain2: Rater 2's domain 2 score; only essays in set 2 have this\n",
    "- domain2_score: Resolved score between the raters; only essays in set 2 have this\n",
    "- rater1_trait1 score - rater3_trait6 score: trait scores for sets 7-8\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pg_37Ffqu99r"
   },
   "source": [
    "### DATA Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQXkDgYIvGm-"
   },
   "source": [
    "#### There are 12976 essays in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1024,
     "status": "ok",
     "timestamp": 1564374539152,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "Dg9h-Y8dubTe",
    "outputId": "e021a5fe-aeb0-4dc8-8619-fb3961c23141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12976 entries, 0 to 12975\n",
      "Data columns (total 28 columns):\n",
      "essay_id          12976 non-null int64\n",
      "essay_set         12976 non-null int64\n",
      "essay             12976 non-null object\n",
      "rater1_domain1    12976 non-null int64\n",
      "rater2_domain1    12976 non-null int64\n",
      "rater3_domain1    128 non-null float64\n",
      "domain1_score     12976 non-null int64\n",
      "rater1_domain2    1800 non-null float64\n",
      "rater2_domain2    1800 non-null float64\n",
      "domain2_score     1800 non-null float64\n",
      "rater1_trait1     2292 non-null float64\n",
      "rater1_trait2     2292 non-null float64\n",
      "rater1_trait3     2292 non-null float64\n",
      "rater1_trait4     2292 non-null float64\n",
      "rater1_trait5     723 non-null float64\n",
      "rater1_trait6     723 non-null float64\n",
      "rater2_trait1     2292 non-null float64\n",
      "rater2_trait2     2292 non-null float64\n",
      "rater2_trait3     2292 non-null float64\n",
      "rater2_trait4     2292 non-null float64\n",
      "rater2_trait5     723 non-null float64\n",
      "rater2_trait6     723 non-null float64\n",
      "rater3_trait1     128 non-null float64\n",
      "rater3_trait2     128 non-null float64\n",
      "rater3_trait3     128 non-null float64\n",
      "rater3_trait4     128 non-null float64\n",
      "rater3_trait5     128 non-null float64\n",
      "rater3_trait6     128 non-null float64\n",
      "dtypes: float64(22), int64(5), object(1)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv(os.path.join(DATA_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8LYOAqRvFJ7"
   },
   "source": [
    "#### There are 8 essay prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 826,
     "status": "ok",
     "timestamp": 1564374544046,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "NoWWiNcpvP3R",
    "outputId": "058842ac-4c2a-44cd-feeb-3e22958d3775"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6, 7, 8}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(X['essay_set'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dSLWPykvyzx"
   },
   "source": [
    "#### Number of null entries by column. Most columns don't have data. We will drop those columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1197,
     "status": "ok",
     "timestamp": 1564374550432,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "1hYNCK2wvwvV",
    "outputId": "707b62fd-4744-4fb3-8f3c-9a5e2a4a3c80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay_id              0\n",
       "essay_set             0\n",
       "essay                 0\n",
       "rater1_domain1        0\n",
       "rater2_domain1        0\n",
       "rater3_domain1    12848\n",
       "domain1_score         0\n",
       "rater1_domain2    11176\n",
       "rater2_domain2    11176\n",
       "domain2_score     11176\n",
       "rater1_trait1     10684\n",
       "rater1_trait2     10684\n",
       "rater1_trait3     10684\n",
       "rater1_trait4     10684\n",
       "rater1_trait5     12253\n",
       "rater1_trait6     12253\n",
       "rater2_trait1     10684\n",
       "rater2_trait2     10684\n",
       "rater2_trait3     10684\n",
       "rater2_trait4     10684\n",
       "rater2_trait5     12253\n",
       "rater2_trait6     12253\n",
       "rater3_trait1     12848\n",
       "rater3_trait2     12848\n",
       "rater3_trait3     12848\n",
       "rater3_trait4     12848\n",
       "rater3_trait5     12848\n",
       "rater3_trait6     12848\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHHK3pHJztOv"
   },
   "source": [
    "#### For essay score, we will retain the domain1_score (this is the resolved score between the 2 raters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2u8gw7dbutFv"
   },
   "outputs": [],
   "source": [
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8R749fcz51m"
   },
   "source": [
    "#### Here is our cleaned out dataset (with 4 columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1242,
     "status": "ok",
     "timestamp": 1564374561073,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "LkYD-RKvQ_z-",
    "outputId": "f242c3c6-459d-44c5-a783-d3ed5357fa33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12976 entries, 0 to 12975\n",
      "Data columns (total 4 columns):\n",
      "essay_id         12976 non-null int64\n",
      "essay_set        12976 non-null int64\n",
      "essay            12976 non-null object\n",
      "domain1_score    12976 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 405.6+ KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1564374564351,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "lQGuQnomytaA",
    "outputId": "11674161-b936-4190-981f-86981dad8713"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IE8j1kNV1kvk"
   },
   "source": [
    "#### Distribution of essay type. Mostly balanced across essay prompts 1-7, except prompt 8 (about half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1564374569513,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "4pqUhr451jct",
    "outputId": "54939548-623f-4481-920a-444488077c23"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay_set\n",
       "1    1783\n",
       "2    1800\n",
       "3    1726\n",
       "4    1770\n",
       "5    1805\n",
       "6    1800\n",
       "7    1569\n",
       "8     723\n",
       "Name: essay, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.groupby('essay_set').agg('count')['essay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1459,
     "status": "ok",
     "timestamp": 1564374574427,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "4ba6-sNb2OHx",
    "outputId": "9ed1ad39-a751-4186-f31b-b326887c54e5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X.groupby('essay_set').agg('count')['essay'].plot.bar()\n",
    "plt.title('Essay count by prompt')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Prompt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zsf1FxNF55X4"
   },
   "source": [
    "#### Essay length (number of words) distribution by prompt after removing the stop words and punctuation (\".\", \"!\", \"@\" etc.).  Here is a list of stopwords removed from the essay using nltk stopwords. \n",
    "\n",
    "[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u8GBoaoj5-ro"
   },
   "outputs": [],
   "source": [
    "X['tokenized_sents'] = X.apply(lambda row: essay_to_wordlist(row['essay'], remove_stopwords = True), axis=1)\n",
    "X['sents_length'] = X.apply(lambda row: len(row['tokenized_sents']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHtudSYBR7p_"
   },
   "source": [
    "#### Average essay length varies from 58-285 words (after removing the stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1385,
     "status": "ok",
     "timestamp": 1564374642926,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "V3EjLcfrJ6GT",
    "outputId": "2339de74-a860-40e9-f3ea-049a01f3608f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>427</td>\n",
       "      <td>183.833988</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>524</td>\n",
       "      <td>175.451667</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>189</td>\n",
       "      <td>52.672654</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>188</td>\n",
       "      <td>43.473446</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>217</td>\n",
       "      <td>58.801662</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>239</td>\n",
       "      <td>82.910556</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>291</td>\n",
       "      <td>76.685787</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>515</td>\n",
       "      <td>285.006916</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           min  max        mean  median\n",
       "essay_set                              \n",
       "1            3  427  183.833988     181\n",
       "2           15  524  175.451667     167\n",
       "3            2  189   52.672654      48\n",
       "4            1  188   43.473446      39\n",
       "5            2  217   58.801662      56\n",
       "6            2  239   82.910556      84\n",
       "7            1  291   76.685787      69\n",
       "8            3  515  285.006916     292"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.groupby(['essay_set'])['sents_length'].agg(['min','max', 'mean', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_KnNWzRQcKfM"
   },
   "source": [
    "#### Lets find the 50, 75 and 90% length stats by essay prompt. \n",
    "#### We will use this for setting maxlen parameter later in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1564374647090,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "IpFU0scycYXI",
    "outputId": "0323dffc-456b-4814-981e-6947346de700"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "essay_set      \n",
       "1          0.50    181.0\n",
       "           0.75    222.5\n",
       "           0.90    265.0\n",
       "2          0.50    167.0\n",
       "           0.75    217.0\n",
       "           0.90    276.0\n",
       "3          0.50     48.0\n",
       "           0.75     71.0\n",
       "           0.90     90.5\n",
       "4          0.50     39.0\n",
       "           0.75     59.0\n",
       "           0.90     79.0\n",
       "5          0.50     56.0\n",
       "           0.75     77.0\n",
       "           0.90     99.0\n",
       "6          0.50     84.0\n",
       "           0.75    103.0\n",
       "           0.90    122.0\n",
       "7          0.50     69.0\n",
       "           0.75     98.0\n",
       "           0.90    134.2\n",
       "8          0.50    292.0\n",
       "           0.75    366.0\n",
       "           0.90    404.0\n",
       "Name: sents_length, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.groupby(['essay_set'])['sents_length'].quantile([0.5, 0.75, 0.90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0U_7Htb_zQg"
   },
   "source": [
    "#### Word count distribution by essay prompt ranges from 200-500 words after cleaning up stopwords and punctuations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2613,
     "status": "ok",
     "timestamp": 1564374779644,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "ysEvt0PkC_gF",
    "outputId": "af895707-ef12-492f-829e-1fd5b88c5ac6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#X.hist(column='sents_length', by='essay', bins=25, sharey=True, sharex=True, layout=(2, 4), figsize=(7,4), rot=0) \n",
    "X.hist(column='sents_length', by='essay_set', sharex=True, sharey=True, layout=(2, 4), figsize=(8,8)) \n",
    "plt.suptitle('Word count by essay prompt')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f1VpLJY_P8mn"
   },
   "source": [
    "#### There is a good score correlation to essay length (most of them > 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1307,
     "status": "ok",
     "timestamp": 1564374791279,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "PhvqKPl_QEMS",
    "outputId": "3f1a1e94-75c2-4e1b-b35b-b57b90ea80a6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sents_length</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>sents_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.813735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <td>0.813735</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>sents_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.673110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <td>0.673110</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>sents_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.706035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <td>0.706035</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>sents_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <td>0.739430</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>sents_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.811706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <td>0.811706</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">6</th>\n",
       "      <th>sents_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <td>0.697972</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">7</th>\n",
       "      <th>sents_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.679424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <td>0.679424</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">8</th>\n",
       "      <th>sents_length</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <td>0.533699</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         sents_length  domain1_score\n",
       "essay_set                                           \n",
       "1         sents_length       1.000000       0.813735\n",
       "          domain1_score      0.813735       1.000000\n",
       "2         sents_length       1.000000       0.673110\n",
       "          domain1_score      0.673110       1.000000\n",
       "3         sents_length       1.000000       0.706035\n",
       "          domain1_score      0.706035       1.000000\n",
       "4         sents_length       1.000000       0.739430\n",
       "          domain1_score      0.739430       1.000000\n",
       "5         sents_length       1.000000       0.811706\n",
       "          domain1_score      0.811706       1.000000\n",
       "6         sents_length       1.000000       0.697972\n",
       "          domain1_score      0.697972       1.000000\n",
       "7         sents_length       1.000000       0.679424\n",
       "          domain1_score      0.679424       1.000000\n",
       "8         sents_length       1.000000       0.533699\n",
       "          domain1_score      0.533699       1.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.groupby('essay_set')[['sents_length','domain1_score']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EkXcAL0PytaF"
   },
   "source": [
    "#### Score distribution for each essay set. Scoring is different for each prompt. It makes sense to train each prompt separately as each essay has differences in grade level, type of essay (narrative, argumentative, source based), and marking differences between each prompt. As mentioned earlier, we will focus on \"essay_set == 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1564374815845,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "nV7xvvmMni5L",
    "outputId": "a75030e7-13bc-4f1b-c0d9-80bcc00d8ad8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>essay_set</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>8.528323</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3.415556</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.848204</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.432203</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.408864</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>16.062460</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>36.950207</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           min  max       mean  median\n",
       "essay_set                             \n",
       "1            2   12   8.528323       8\n",
       "2            1    6   3.415556       3\n",
       "3            0    3   1.848204       2\n",
       "4            0    3   1.432203       1\n",
       "5            0    4   2.408864       2\n",
       "6            0    4   2.720000       3\n",
       "7            2   24  16.062460      16\n",
       "8           10   60  36.950207      37"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.groupby(['essay_set'])['domain1_score'].agg(['min','max', 'mean', 'median'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfcyGswqytaP"
   },
   "source": [
    "#### For this project we will consider \"prompt 3\" essay (essay_set == 3)\n",
    "#### This is one of the harder prompts as numerous papers have reported low cohen-kappa scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYnKYwmSytaY"
   },
   "outputs": [],
   "source": [
    "X_prompt3 = X[X['essay_set'] == 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TPEWcleT4Sdf"
   },
   "source": [
    "#### Most samples are scored 1, 2 and 3, and very few zero scores. So, class 0 is unbalanced. \n",
    "#### We will use stratified k-fold to ensure the training set contains a balanced class 0 in each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1564374886837,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "Cj0Xq2M02q1_",
    "outputId": "40d4aacd-bb91-4745-fe1b-cdc0ef4c45e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count\n",
       "domain1_score       \n",
       "0                 39\n",
       "1                607\n",
       "2                657\n",
       "3                423"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_prompt3.groupby(['domain1_score'])['essay_id'].agg(['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All models below are using stratified k-fold to keep a balanced representation of classes during training, and evaluation. We have picked maxlen=100 words, as 95% of prompt 3 essays are below 100 words in length. We have picked a vocab size of 4000 words to match the results in the 2 papers listed above.\n",
    "#### Evaluation is being done with cohen-kappa score (measure of goodness of model's score with human score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIhjxoTKt085"
   },
   "source": [
    "### Training and evaluation of CNN Model with tf-idf representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 448539,
     "status": "ok",
     "timestamp": 1564377167760,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "l2ihFzI1t8jd",
    "outputId": "f2c53967-7823-44ad-a31c-763ac5b3f08d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3\n",
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, 3998, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1999, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 1997, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 998, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 31936)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               16351744  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 16,487,332\n",
      "Trainable params: 16,487,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Kappa Score: 0.5594461187973137\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Kappa Score: 0.4522615289292775\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Kappa Score: 0.4937302791930923\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Kappa Score: 0.4901371509753565\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Kappa Score: 0.4231333509865526\n",
      "Average Kappa score after a 5-fold cross validation:  0.4837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "from keras.layers import Embedding, Dense, Dropout, Flatten\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, AveragePooling1D, TimeDistributed\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "skfold = StratifiedKFold(5, True, 1)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "count = 1\n",
    "\n",
    "#X_prompt = p3_df\n",
    "X_prompt = X_prompt3\n",
    "y = X_prompt['domain1_score']\n",
    "#y = y.values\n",
    "\n",
    "maxlen = 100\n",
    "vocab_size = 4000\n",
    "embedding_dim = 300\n",
    "keras_tokenizer = keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token='UNK')\n",
    "keras_tokenizer.fit_on_texts(X_prompt['essay'])\n",
    "encoded_essays = np.array(keras_tokenizer.texts_to_matrix(X_prompt['essay'], mode='tfidf'))\n",
    "#print(f'encoded_docs size={len(encoded_docs)}, encoded_docs={encoded_docs}')\n",
    "\n",
    "print(f'Prompt 3')\n",
    "for traincv, testcv in skfold.split(X_prompt,y):\n",
    "  print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "  X_test, X_train, y_test, y_train = X_prompt.iloc[testcv], X_prompt.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "  train_essays = X_train['essay']\n",
    "  test_essays = X_test['essay']\n",
    "  \n",
    "  y_train = y_train.values\n",
    "  y_test = y_test.values\n",
    "  \n",
    "  #print(f'y_train={y_train}')\n",
    "  encoded_essays = encoded_essays.reshape(encoded_essays.shape[0], encoded_essays.shape[1], 1)\n",
    "  input_train_essays = encoded_essays[traincv]\n",
    "  input_test_essays = encoded_essays[testcv]\n",
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(vocab_size, 1)))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "  if (count == 1):  \n",
    "    model.summary()\n",
    "  history = model.fit(input_train_essays, y_train, batch_size=64, epochs=20, verbose=0)\n",
    "\n",
    "  y_pred = model.predict(input_test_essays)\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "  # Round y_pred to the nearest integer.\n",
    "  #y_pred = np.around(y_pred)\n",
    "    \n",
    "  # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "  #result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "  result = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
    "  print(\"Kappa Score: {}\".format(result))\n",
    "  results.append(result)\n",
    "    \n",
    "  count += 1\n",
    "    \n",
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))  \n",
    "#results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see average kappa score of 0.48. The best models have reported scores from 0.695-0.78.\n",
    "#### Please see these 2 reference papers \n",
    "* Incorporating Neural Coherence Features: https://arxiv.org/abs/1711.04981\n",
    "* A Siamese Bidirectional LSTM Architecture: https://www.mdpi.com/2073-8994/10/12/682\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSRtc6XofT2g"
   },
   "source": [
    "### Lets see if we can improve our kappa scores with CNN+jointly learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 250127,
     "status": "ok",
     "timestamp": 1564376502772,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "CnEm3vNxfXo6",
    "outputId": "a40249ef-679a-411c-8115-a02358c632fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3\n",
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 300)          2100000   \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 98, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 49, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 47, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 736)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 300)               221100    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 2,398,790\n",
      "Trainable params: 2,398,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[ 1  6  0  1]\n",
      " [ 3 78 39  2]\n",
      " [ 2 25 87 18]\n",
      " [ 1  4 48 32]]\n",
      "Kappa Score: 0.53946786323706\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "[[ 2  5  1  0]\n",
      " [ 4 71 43  4]\n",
      " [ 0 21 82 29]\n",
      " [ 0  5 55 25]]\n",
      "Kappa Score: 0.5146508573254287\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "[[  1   7   0   0]\n",
      " [  3  78  33   7]\n",
      " [  1  17 101  12]\n",
      " [  0   7  64  14]]\n",
      "Kappa Score: 0.47604140838085796\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "[[ 0  6  2  0]\n",
      " [ 5 73 38  5]\n",
      " [ 0 24 86 21]\n",
      " [ 0  2 55 27]]\n",
      "Kappa Score: 0.5327047904367492\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "[[ 4  1  1  1]\n",
      " [ 3 70 34 14]\n",
      " [ 0 24 62 45]\n",
      " [ 0  5 35 44]]\n",
      "Kappa Score: 0.500028395646001\n",
      "Average Kappa score after a 5-fold cross validation:  0.5126\n"
     ]
    }
   ],
   "source": [
    "skfold = StratifiedKFold(5, True, 1)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "count = 1\n",
    "\n",
    "#X_prompt = p3_df\n",
    "X_prompt = X_prompt3\n",
    "y = X_prompt['domain1_score']\n",
    "#y = y.values\n",
    "\n",
    "maxlen = 100\n",
    "vocab_size = 7000\n",
    "embedding_dim = 300\n",
    "keras_tokenizer = keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token='UNK')\n",
    "keras_tokenizer.fit_on_texts(X_prompt['essay'])\n",
    "sequences = keras_tokenizer.texts_to_sequences(X_prompt['essay'])\n",
    "encoded_essays = sequence.pad_sequences(sequences, maxlen=100, padding='post')\n",
    "\n",
    "print(f'Prompt 3')\n",
    "for traincv, testcv in skfold.split(X_prompt, y):\n",
    "  print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "  X_test, X_train, y_test, y_train = X_prompt.iloc[testcv], X_prompt.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "  train_essays = X_train['essay']\n",
    "  test_essays = X_test['essay']\n",
    "  \n",
    "  y_train = y_train.values\n",
    "  y_test = y_test.values\n",
    "  \n",
    "  #print(f'y_train={y_train}')  \n",
    "  input_train_essays = encoded_essays[traincv]\n",
    "  input_test_essays = encoded_essays[testcv]\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  #model.add(Bidirectional(LSTM(300, dropout=0.4, recurrent_dropout=0.4), merge_mode='ave'))#, return_sequences=True))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(300, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(150, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  if (count == 1):\n",
    "    model.summary()\n",
    "  history = model.fit(input_train_essays, y_train, batch_size=64, epochs=20, verbose=0)\n",
    "\n",
    "  y_pred = model.predict(input_test_essays)\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "  matrix = confusion_matrix(y_test, y_pred)\n",
    "  print(matrix)\n",
    "    \n",
    "  # Round y_pred to the nearest integer.\n",
    "  #y_pred = np.around(y_pred)\n",
    "    \n",
    "  # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "  #result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "  result = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
    "  print(\"Kappa Score: {}\".format(result))\n",
    "  results.append(result)\n",
    "    \n",
    "  count += 1\n",
    "    \n",
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scores improved from 0.48 to 0.51 with jointly learned embeddings. Confusion matrix show large number of errors for a score of 0. This is because our training sample set for zero scores is low. Let's see if we can improve the model with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have leveraged the data augmentation code from https://github.com/jasonwei20/eda_nlp?source=post_page---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please see this blog post https://towardsdatascience.com/these-are-the-easiest-data-augmentation-techniques-in-natural-language-processing-you-can-think-of-88e393fd610\n",
    "\n",
    "It describes the following techniques, and our model uses all of them:\n",
    "* Synonym Replacement (SR): Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\n",
    "* Random Insertion (RI): Find a random synonym of a random word in the sentence that is not a stop word. Insert that synonym into a random position in the sentence. Do this n times.\n",
    "* Random Swap (RS): Randomly choose two words in the sentence and swap their positions. Do this n times.\n",
    "* Random Deletion (RD): For each word in the sentence, randomly remove it with probability p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHcwAgtj8Wap"
   },
   "source": [
    "Lets get the zero score samples from prompt 3 essay set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vcg71DYV8Vja"
   },
   "outputs": [],
   "source": [
    "p3_df = X_prompt3[['domain1_score', 'essay']]\n",
    "p3_df_zeroscore = p3_df[p3_df['domain1_score'] == 0]\n",
    "p3_df_zeroscore.reset_index(inplace=True)\n",
    "p3_df_zeroscore = p3_df_zeroscore[['domain1_score', 'essay']]\n",
    "p3_df_zeroscore.to_csv(DATA_DIR + '/essay.tsv', \n",
    "             sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bAQUroC5fxAY"
   },
   "source": [
    "### Data Augmentation Routines from Jason Wei and Kai Zou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2027,
     "status": "ok",
     "timestamp": 1564376108537,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "MC9l8eabdVDx",
    "outputId": "0694018d-c00b-4ba7-aa5e-7cbfc2560ff1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input=./data/essay.tsv\n",
      "output=./data/eda_essay.tsv\n",
      "generated augmented sentences with eda for ./data/essay.tsv to ./data/eda_essay.tsv with num_aug=9\n"
     ]
    }
   ],
   "source": [
    "# Jason Wei and Kai Zou\n",
    "\n",
    "#number of augmented sentences to generate per original sentence\n",
    "num_aug = 9 #default\n",
    "#how much to change each sentence\n",
    "alpha = 0.1#default\n",
    "\n",
    "\n",
    "\n",
    "#stop words list\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
    "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
    "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
    "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
    "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
    "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
    "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
    "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "\t\t\t'should', 'now', '']\n",
    "\n",
    "#cleaning up text\n",
    "\n",
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line\n",
    "\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word): \n",
    "\t\tfor l in syn.lemmas(): \n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym) \n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "\n",
    "def random_deletion(words, p):\n",
    "\n",
    "\t#obviously, if there's only one word, don't delete it\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\t#randomly delete words with probability p\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\t#if you end up deleting all words, just return a random word\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random insertion\n",
    "# Randomly insert n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "def random_insertion(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\n",
    "########################################################################\n",
    "# main data augmentation function\n",
    "########################################################################\n",
    "\n",
    "def eda(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=9):\n",
    "\t\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word is not '']\n",
    "\tnum_words = len(words)\n",
    "\t\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4)+1\n",
    "\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "\t#sr\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#ri\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_insertion(words, n_ri)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rs\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_swap(words, n_rs)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rd\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\t#trim so that we have the desired number of augmented sentences\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\t#append the original sentence\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences\n",
    "\n",
    "\n",
    "\n",
    "#generate more data with standard augmentation\n",
    "def gen_eda(train_orig, output_file, alpha, num_aug=9):\n",
    "\n",
    "    writer = open(output_file, 'w')\n",
    "    lines = open(train_orig, 'r').readlines()\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        parts = line[:-1].split('\\t')\n",
    "        label = parts[0]\n",
    "        sentence = parts[1]\n",
    "        aug_sentences = eda(sentence, alpha_sr=alpha, alpha_ri=alpha, alpha_rs=alpha, p_rd=alpha, num_aug=num_aug)\n",
    "        for aug_sentence in aug_sentences:\n",
    "            writer.write(label + \"\\t\" + aug_sentence + '\\n')\n",
    "\n",
    "    writer.close()\n",
    "    print(\"generated augmented sentences with eda for \" + train_orig + \" to \" + output_file + \" with num_aug=\" + str(num_aug))\n",
    "\n",
    "\n",
    "#generate augmented sentences and output into a new file \"eda_essay.tsv\"\n",
    "output = DATA_DIR + '/eda_essay.tsv'\n",
    "input = DATA_DIR + '/essay.tsv'\n",
    "print(f'input={input}')\n",
    "print(f'output={output}')\n",
    "\n",
    "gen_eda(input, output, alpha=alpha, num_aug=num_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3R7ymhmF9lN"
   },
   "source": [
    "lets read the data augmented file \"eda_essay.tsv\" which contains augmented samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cE0yqVQiF8ly"
   },
   "outputs": [],
   "source": [
    "colnames=['domain1_score', 'essay']\n",
    "p3_aug_df = pd.read_csv(DATA_DIR + '/eda_essay.tsv',\n",
    "                       sep='\\t', names=colnames, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have increased our samples from 39 to 390 zero scored samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 586,
     "status": "ok",
     "timestamp": 1564376150319,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "c8FPgdkhGXTa",
    "outputId": "297cef88-6187-4efe-a1bc-4cb040fc524b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 390 entries, 0 to 389\n",
      "Data columns (total 2 columns):\n",
      "domain1_score    390 non-null int64\n",
      "essay            390 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 6.2+ KB\n"
     ]
    }
   ],
   "source": [
    "p3_aug_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add the augmented zero scored samples to original data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 933,
     "status": "ok",
     "timestamp": 1564376192915,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "xTwdQcH78JEc",
    "outputId": "eec05f99-1b03-4207-cd1f-5e7dc96666c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1687 entries, 3583 to 5308\n",
      "Data columns (total 6 columns):\n",
      "essay_id           1687 non-null int64\n",
      "essay_set          1687 non-null int64\n",
      "essay              1687 non-null object\n",
      "domain1_score      1687 non-null int64\n",
      "tokenized_sents    1687 non-null object\n",
      "sents_length       1687 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 92.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The features of the setting affect the cyclist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The features of the setting affected the cycli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Everyone travels to unfamiliar places. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>I believe the features of the cyclist affected...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>The setting effects the cyclist because of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain1_score                                              essay\n",
       "0              1  The features of the setting affect the cyclist...\n",
       "1              2  The features of the setting affected the cycli...\n",
       "2              1  Everyone travels to unfamiliar places. Sometim...\n",
       "3              1  I believe the features of the cyclist affected...\n",
       "4              2  The setting effects the cyclist because of the..."
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nonzeroscore = X_prompt3[X_prompt3['domain1_score'] != 0]\n",
    "X_nonzeroscore.info()\n",
    "p3_df_nonzeroscore = X_nonzeroscore[['domain1_score', 'essay']]\n",
    "p3_df_nonzeroscore.reset_index(inplace=True)\n",
    "p3_df_nonzeroscore = p3_df_nonzeroscore[['domain1_score', 'essay']]\n",
    "p3_df_nonzeroscore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1564376220639,
     "user": {
      "displayName": "Amit Gupta",
      "photoUrl": "",
      "userId": "06429676738067602298"
     },
     "user_tz": 420
    },
    "id": "qQIQ2zrFKshV",
    "outputId": "5a7ca345-33fc-4d55-853d-ec6ee8e47271"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domain1_score</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              essay\n",
       "              count\n",
       "domain1_score      \n",
       "0               390\n",
       "1               607\n",
       "2               657\n",
       "3               423"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p3_df = pd.concat([p3_df_nonzeroscore, p3_aug_df], axis = 0)\n",
    "p3_df.groupby(['domain1_score']).agg(['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets retrain our CNN model with tf-idf representation with augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3\n",
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_25 (Conv1D)           (None, 3998, 32)          128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 1999, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 1997, 32)          3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 998, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 31936)             0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 512)               16351744  \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 16,487,332\n",
      "Trainable params: 16,487,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Kappa Score: 0.7834034410650993\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Kappa Score: 0.74865610093357\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Kappa Score: 0.7473147856368405\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Kappa Score: 0.7752186731386315\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Kappa Score: 0.7474162736451399\n",
      "Average Kappa score after a 5-fold cross validation:  0.7604\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "from keras.layers import Embedding, Dense, Dropout, Flatten\n",
    "from keras.layers import LSTM, Conv1D, MaxPooling1D, AveragePooling1D, TimeDistributed\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "skfold = StratifiedKFold(5, True, 1)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "count = 1\n",
    "\n",
    "X_prompt = p3_df\n",
    "y = X_prompt['domain1_score']\n",
    "#y = y.values\n",
    "\n",
    "maxlen = 100\n",
    "vocab_size = 4000\n",
    "embedding_dim = 300\n",
    "keras_tokenizer = keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token='UNK')\n",
    "keras_tokenizer.fit_on_texts(X_prompt['essay'])\n",
    "encoded_essays = np.array(keras_tokenizer.texts_to_matrix(X_prompt['essay'], mode='tfidf'))\n",
    "#print(f'encoded_docs size={len(encoded_docs)}, encoded_docs={encoded_docs}')\n",
    "\n",
    "print(f'Prompt 3')\n",
    "for traincv, testcv in skfold.split(X_prompt,y):\n",
    "  print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "  X_test, X_train, y_test, y_train = X_prompt.iloc[testcv], X_prompt.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "  train_essays = X_train['essay']\n",
    "  test_essays = X_test['essay']\n",
    "  \n",
    "  y_train = y_train.values\n",
    "  y_test = y_test.values\n",
    "  \n",
    "  #print(f'y_train={y_train}')\n",
    "  encoded_essays = encoded_essays.reshape(encoded_essays.shape[0], encoded_essays.shape[1], 1)\n",
    "  input_train_essays = encoded_essays[traincv]\n",
    "  input_test_essays = encoded_essays[testcv]\n",
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(vocab_size, 1)))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(512, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "  if (count == 1):  \n",
    "    model.summary()\n",
    "  history = model.fit(input_train_essays, y_train, batch_size=64, epochs=20, verbose=0)\n",
    "\n",
    "  y_pred = model.predict(input_test_essays)\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "  # Round y_pred to the nearest integer.\n",
    "  #y_pred = np.around(y_pred)\n",
    "    \n",
    "  # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "  #result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "  result = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
    "  print(\"Kappa Score: {}\".format(result))\n",
    "  results.append(result)\n",
    "    \n",
    "  count += 1\n",
    "    \n",
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))  \n",
    "#results = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We see a significant improvement in kappa from 0.48 to 0.76 with data augmention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check our results of CNN model with embeddings and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3\n",
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 100, 300)          2100000   \n",
      "_________________________________________________________________\n",
      "conv1d_35 (Conv1D)           (None, 98, 32)            28832     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 49, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_36 (Conv1D)           (None, 47, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 736)               0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 300)               221100    \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 2,398,790\n",
      "Trainable params: 2,398,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[[78  0  0  0]\n",
      " [ 6 77 33  6]\n",
      " [ 1 22 86 23]\n",
      " [ 0  5 62 18]]\n",
      "Kappa Score: 0.7574621943468407\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "[[77  1  0  0]\n",
      " [ 7 81 30  4]\n",
      " [ 2 27 90 13]\n",
      " [ 2  4 66 13]]\n",
      "Kappa Score: 0.7391037712473363\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "[[ 75   3   0   0]\n",
      " [  3  68  46   4]\n",
      " [  0  11 102  18]\n",
      " [  0   5  56  24]]\n",
      "Kappa Score: 0.7784084420534758\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "[[78  0  0  0]\n",
      " [ 4 76 35  6]\n",
      " [ 0 20 61 50]\n",
      " [ 0  9 36 39]]\n",
      "Kappa Score: 0.7677759353811299\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "[[78  0  0  0]\n",
      " [ 3 74 42  2]\n",
      " [ 1 30 91  9]\n",
      " [ 1  6 66 11]]\n",
      "Kappa Score: 0.7379661787140122\n",
      "Average Kappa score after a 5-fold cross validation:  0.7561\n"
     ]
    }
   ],
   "source": [
    "skfold = StratifiedKFold(5, True, 1)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "count = 1\n",
    "\n",
    "X_prompt = p3_df\n",
    "y = X_prompt['domain1_score']\n",
    "#y = y.values\n",
    "\n",
    "maxlen = 100\n",
    "vocab_size = 7000\n",
    "embedding_dim = 300\n",
    "keras_tokenizer = keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token='UNK')\n",
    "keras_tokenizer.fit_on_texts(X_prompt['essay'])\n",
    "sequences = keras_tokenizer.texts_to_sequences(X_prompt['essay'])\n",
    "encoded_essays = sequence.pad_sequences(sequences, maxlen=100, padding='post')\n",
    "\n",
    "print(f'Prompt 3')\n",
    "for traincv, testcv in skfold.split(X_prompt, y):\n",
    "  print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "  X_test, X_train, y_test, y_train = X_prompt.iloc[testcv], X_prompt.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "  train_essays = X_train['essay']\n",
    "  test_essays = X_test['essay']\n",
    "  \n",
    "  y_train = y_train.values\n",
    "  y_test = y_test.values\n",
    "  \n",
    "  #print(f'y_train={y_train}')  \n",
    "  input_train_essays = encoded_essays[traincv]\n",
    "  input_test_essays = encoded_essays[testcv]\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Conv1D(filters=32, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  #model.add(Bidirectional(LSTM(300, dropout=0.4, recurrent_dropout=0.4), merge_mode='ave'))#, return_sequences=True))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(300, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(150, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(4, activation='softmax'))\n",
    "  \n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  if (count == 1):\n",
    "    model.summary()\n",
    "  history = model.fit(input_train_essays, y_train, batch_size=64, epochs=20, verbose=0)\n",
    "\n",
    "  y_pred = model.predict(input_test_essays)\n",
    "  y_pred = np.argmax(y_pred, axis=1)\n",
    "  matrix = confusion_matrix(y_test, y_pred)\n",
    "  print(matrix)\n",
    "    \n",
    "  # Round y_pred to the nearest integer.\n",
    "  #y_pred = np.around(y_pred)\n",
    "    \n",
    "  # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "  #result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "  result = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
    "  print(\"Kappa Score: {}\".format(result))\n",
    "  results.append(result)\n",
    "    \n",
    "  count += 1\n",
    "    \n",
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kappa Scores improved from 0.51 (without augentation) to 0.756 (with augmentation).\n",
    "So, data augmentation techniques can be effective with unbalanced classes or small sample sizes (which was the case here). However, caution is warranted when extrapolating/generalizing the results. Since data augmentation takes the same essay, and creates a variation, the models learn the variations. However with a new essay, the results may not improve as much. In such cases, the 2 papers listed show promising approaches to better understand the logical progression and semantic relatedness to the main prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dlt-finalproject.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
